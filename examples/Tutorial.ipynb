{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using **pytwanalysis** - (**TwitterAnalysis**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [PIP Package](https://pypi.org/project/pytwanalysis/)\n",
    "+ [Documentation](https://lianogueira.github.io/pytwanalysis-documentation/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytwanalysis as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set your mongoDB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "#db connection\n",
    "mongoDBConnectionSTR = \"mongodb://localhost:27017\"\n",
    "client = MongoClient(mongoDBConnectionSTR)\n",
    "db = client.twitter_DB_API_test1 #choose your DB name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the folder path you want to save all of the ouput files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'D:\\\\Data\\\\MyFiles3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize your twitterAnalysis object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnalysis = ta.TwitterAnalysis(BASE_PATH, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from json files into the mongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the folder path where all of your twitter json files should be\n",
    "JSON_FILES_PATH = 'D:\\\\Data\\\\tests\\\\my_json_files'\n",
    "\n",
    "# Load json files into mongoDB\n",
    "myAnalysis.loadDocFromFile(JSON_FILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request data from Twitter's 7-day Search API\n",
    "-API endpoint: https://api.twitter.com/1.1/search/tweets.json\n",
    "\n",
    "-[Twitter Search API documentation](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you authentication keys here - (you can retrive these from your Twitter's developer account)\n",
    "consumer_key = '[your consumer_key]'\n",
    "consumer_secret = '[yourconsumer_secret]'\n",
    "access_token = '[your access_token]'\n",
    "access_token_secret = '[your access_token_secret]'\n",
    "query='term1 OR term2 OR love'\n",
    "\n",
    "# send the request to Twitter and save data into MongoDB\n",
    "response = myAnalysis.search7dayapi(consumer_key, consumer_secret, access_token, access_token_secret, query, result_type= 'mixed', max_count='100', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request data from Twitter's Premium Search API\n",
    "30-day API endpoint: https://api.twitter.com/1.1/tweets/search/30day/\n",
    "\n",
    "fullarchive API endpoint: https://api.twitter.com/1.1/tweets/search/fullarchive/\n",
    "\n",
    "-[Twitter Search API documentation](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJtYXhJZCI6MTIyODgzMTI5ODU4OTk4NjgxNn0=\n"
     ]
    }
   ],
   "source": [
    "# options are \"30day\" or fullarchive\n",
    "api_name = \"fullarchive\"\n",
    "\n",
    "# the name of your dev environment - (The one associate with your Twitter developer account)\n",
    "dev_environment = \"FullArchDev.json\"\n",
    "\n",
    "# your query\n",
    "query = \"(term1 OR term2 OR term3) lang:en\"\n",
    "\n",
    "# start and end date\n",
    "date_start = \"202002150000\"\n",
    "date_end = \"202002160000\"\n",
    "\n",
    "# twitter bearear authentication - (this can be generated from your authentication keys)\n",
    "twitter_bearer = '[your bearer token]'\n",
    "\n",
    "# send the request to Twitter and save data into MongoDB\n",
    "response, next_token = myAnalysis.searchPremiumAPI(twitter_bearer, api_name, dev_environment, query, date_start, date_end, next_token=None, max_count='100')\n",
    "\n",
    "print (next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create database collections that will be used to analyse the data\n",
    "###### *Depending on the size of your data, this could take a while...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set the number of tweets to load at a time. \n",
    "# (Large number may cause out of memory errors, low number may take a long time to run)\n",
    "step = 50000\n",
    "\n",
    "# Build collections\n",
    "myAnalysis.build_db_collections(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export edges from MongoDB\n",
    "###### This step will create edge files that will be used for graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the periods you want to analyze \n",
    "# Set period_arr to None if you don't want to analyze separate periods\n",
    "# Format: Period Name, Period Start Date, Period End Date\n",
    "period_arr = [['P1', '10/08/2017 00:00:00', '10/15/2017 00:00:00'],             \n",
    "              ['P2', '01/21/2018 00:00:00', '02/04/2018 00:00:00'],              \n",
    "              ['P3', '02/04/2018 00:00:00', '02/18/2018 00:00:00'],\n",
    "              ['P4', '02/18/2018 00:00:00', '03/04/2018 00:00:00']]\n",
    "\n",
    "\n",
    "## TYPE OF GRAPH EDGES\n",
    "########################################################\n",
    "# You can export edges for one type, or for all\n",
    "# Options: user_conn_all,       --All user connections\n",
    "#          user_conn_mention,   --Only Mentions user connections\n",
    "#          user_conn_retweet,   --Only Retweets user connections\n",
    "#          user_conn_reply,     --Only Replies user connections\n",
    "#          user_conn_quote,     --Only Quotes user connections\n",
    "#          ht_conn              --Hashtag connects - (Hashtgs that were used together)\n",
    "#          all                  --It will export all of the above options\n",
    "\n",
    "TYPE_OF_GRAPH = 'all'\n",
    "\n",
    "myAnalysis.export_mult_types_edges_for_input(period_arr=period_arr, type_of_graph=TYPE_OF_GRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print initial EDA\n",
    "###### this will show you the summary information about your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnalysis.eda_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation Analysis. \n",
    "##### It creates all folders and analysis files based on your given settings\n",
    "###### \n",
    "## IMPORTANT STEP: Choose your settings here before running the automation analysis\n",
    "##### These variables will help you decide what files you want to see and with which parameters \n",
    "##### Running the analysis step could take a long time. \n",
    "##### If you want to run piece by piece so you can see results soon, you can change the flags to 'Y' one at the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TYPE OF GRAPH ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TYPE OF GRAPH ANALYSIS\n",
    "########################################################\n",
    "# Type of graph analysis\n",
    "# Options: user_conn_all,       --All user connections\n",
    "#          user_conn_mention,   --Only Mentions user connections\n",
    "#          user_conn_retweet,   --Only Retweets user connections\n",
    "#          user_conn_reply,     --Only Replies user connections\n",
    "#          user_conn_quote,     --Only Quotes user connections\n",
    "#          ht_conn              --Hashtag connects - (Hashtgs that were used together)\n",
    "\n",
    "TYPE_OF_GRAPH = 'user_conn_all'\n",
    "#------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OUTPUT PATH, PERIOD AND BOT SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUT PATH, PERIOD AND BOT SETTINGS\n",
    "########################################################\n",
    "\n",
    "# Path where you want to save your output files\n",
    "# It will use the path you already set previously, \n",
    "# but you can change here in case you want a new path\n",
    "OUTPUT_PATH = BASE_PATH  \n",
    "\n",
    "#Filter bots or not. Options: (None, '0', or '1')\n",
    "IS_BOT_FILTER = None\n",
    "\n",
    "# Same period array you already set previously. \n",
    "# You can change here in case you want something new, \n",
    "# just follow the same format as array in previous step\n",
    "PERIOD_ARR =  [['P1', '10/08/2017 00:00:00', '10/15/2017 00:00:00'],             \n",
    "              ['P2', '01/21/2018 00:00:00', '02/04/2018 00:00:00'],              \n",
    "              ['P3', '02/04/2018 00:00:00', '02/18/2018 00:00:00'],\n",
    "              ['P4', '02/18/2018 00:00:00', '03/04/2018 00:00:00']]     \n",
    "#------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FILES TO CREATE OPTIONS\n",
    "###### Choose which files you want to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILES TO CREATE OPTIONS\n",
    "# Choose which files you want to create \n",
    "########################################################\n",
    "\n",
    "\n",
    "# Creates a separate folder for the top degree nodes\n",
    "#------------------------------------------------------------\n",
    "CREATE_TOP_NODES_FILES_FLAG = 'Y'\n",
    "# IF you chose CREATE_TOP_NODES_FILES_FLAG='Y', you can also set these settings\n",
    "# We will create subfolder for the top degree nodes based on these number\n",
    "TOP_DEGREE_START = 1   \n",
    "TOP_DEGREE_END = 25\n",
    "# We will create subfolders for the top degree nodes \n",
    "# for each period based on these numbers\n",
    "PERIOD_TOP_DEGREE_START = 1\n",
    "PERIOD_TOP_DEGREE_END = 10\n",
    "\n",
    "\n",
    "# Creates files with the edges of each folder \n",
    "# and a list of nodes and their degree\n",
    "#------------------------------------------------------------\n",
    "CREATE_NODES_EDGES_FILES_FLAG = 'Y'\n",
    "\n",
    "\n",
    "# Creates the graph visualization files\n",
    "#------------------------------------------------------------\n",
    "CREATE_GRAPHS_FILES_FLAG = 'Y'\n",
    "\n",
    "\n",
    "# Creates files for topic discovery\n",
    "#------------------------------------------------------------\n",
    "# Tweet texts for that folder, word cloud, and LDA Model Visualization\n",
    "CREATE_TOPIC_MODEL_FILES_FLAG = 'Y'\n",
    "# If you chose CREATE_TOPIC_MODEL_FILES_FLAG='Y', you can also set this setting\n",
    "# This is the number of topics to send as input to LDA model (Default is 4)\n",
    "NUM_OF_TOPICS = 4\n",
    "\n",
    "\n",
    "# Creates files with ht frequency\n",
    "#------------------------------------------------------------\n",
    "# Text files with all hashtags used, wordcloud, and barchart\n",
    "CREATE_HT_FREQUENCY_FILES_FLAG = 'Y'\n",
    "\n",
    "\n",
    "# Creates files with word frequency\n",
    "#------------------------------------------------------------\n",
    "# Text files with all hashtags used, wordcloud, and barchart\n",
    "CREATE_WORDS_FREQUENCY_FILES_FLAG = 'Y'\n",
    "# If you answer yes to CREATE_WORDS_FREQUENCY_FILES_FLAG, then you can choose\n",
    "# how many words you want to see in your list file.\n",
    "# The number of words to save on the frequency word list file. (Default=5000)\n",
    "TOP_NO_WORD_FILTER = 5000   \n",
    "\n",
    "\n",
    "# Creates files with time series data\n",
    "#------------------------------------------------------------\n",
    "CREATE_TIMESERIES_FILES_FLAG = 'Y'\n",
    "\n",
    "\n",
    "# Creates graphs with hashtag information\n",
    "#------------------------------------------------------------\n",
    "# This can be used when you're analyzing user connections, \n",
    "# but still want to see the hashtag connection graph for that group of users\n",
    "CREATE_HT_CONN_FILES_FLAG = 'Y'\n",
    "# IF you chose CREATE_HT_CONN_FILES_FLAG = 'Y', you can also set this setting\n",
    "# This is to ignore the top hashtags in the visualization\n",
    "# Sometimes ignoring the main hashtag can be helpful in visualization to\n",
    "# discovery other important structures within the graph\n",
    "TOP_HT_TO_IGNORE = 2\n",
    "\n",
    "\n",
    "# Creates louvain communities folder and files\n",
    "#------------------------------------------------------------\n",
    "CREATE_COMMUNITY_FILES_FLAG = 'N'\n",
    "# If set CREATE_COMMUNITY_FILES_FLAG = 'Y', then you can\n",
    "# set a cutoff number of edges to identify when a folder should be created\n",
    "# If the commty has less edges than this number, it won't create a new folder\n",
    "# Default is 200\n",
    "COMMTY_EDGE_SIZE_CUTOFF = 200 \n",
    "#------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "## GRAPH OPTIONS #######################################\n",
    "########################################################\n",
    "\n",
    "# In case you want to print full graph, with no reduction, and without node scale\n",
    "CREATE_GRAPH_WITHOUT_NODE_SCALE_FLAG = 'Y'\n",
    "# In case you want to print full graph, with no reduction, but with node scale\n",
    "CREATE_GRAPH_WITH_NODE_SCALE_FLAG = 'Y'\n",
    "# In case you want to print reduced graph\n",
    "CREATE_REDUCED_GRAPH_FLAG = 'Y'\n",
    "\n",
    "# This is the cutoff number of edges to decide if we will print \n",
    "# the graph or not. The logic will remove nodes until it can get \n",
    "# to this max number of edges to plot\n",
    "# If you choose a large number it may take a long time to run. \n",
    "# If you choose a small number it may contract nodes too much or not print the graph at all\n",
    "GRAPH_PLOT_CUTOFF_NO_NODES = 3000\n",
    "GRAPH_PLOT_CUTOFF_NO_EDGES = 10000\n",
    "\n",
    "\n",
    "# Reduced Graph settings\n",
    "#------------------------------------------------------------\n",
    "# This is a percentage number used to remove nodes\n",
    "# so we can be able to plot large graphs. \n",
    "# You can run this logic multiple times with different percentages. \n",
    "# Each time the logic will save the graph file with a different name \n",
    "# according to the parameter given\n",
    "REDUCED_GRAPH_COMTY_PER = 90\n",
    "    \n",
    "# Reduce graph by removing edges with weight less than this number\n",
    "# None if you don't want to use this reduction method\n",
    "REDUCED_GRAPH_REMOVE_EDGE_WEIGHT = None   \n",
    "\n",
    "# Continuously reduce graph until it gets to the GRAPH_PLOT_CUTOFF numbers or to 0\n",
    "REDUCED_GRAPH_REMOVE_EDGES_UNTIL_CUTOFF_FLAG = 'Y'\n",
    "#------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UPDATE OBJECT WITH YOUR CHOICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configurations\n",
    "myAnalysis.setConfigs(type_of_graph=TYPE_OF_GRAPH,\n",
    "            is_bot_Filter=IS_BOT_FILTER,\n",
    "            period_arr=PERIOD_ARR,\n",
    "            create_nodes_edges_files_flag=CREATE_NODES_EDGES_FILES_FLAG, \n",
    "            create_graphs_files_flag=CREATE_GRAPHS_FILES_FLAG,\n",
    "            create_topic_model_files_flag=CREATE_TOPIC_MODEL_FILES_FLAG,\n",
    "            create_ht_frequency_files_flag=CREATE_HT_FREQUENCY_FILES_FLAG,\n",
    "            create_words_frequency_files_flag=CREATE_WORDS_FREQUENCY_FILES_FLAG,\n",
    "            create_timeseries_files_flag=CREATE_TIMESERIES_FILES_FLAG,\n",
    "            create_top_nodes_files_flag=CREATE_TOP_NODES_FILES_FLAG, \n",
    "            create_community_files_flag=CREATE_COMMUNITY_FILES_FLAG,\n",
    "            create_ht_conn_files_flag=CREATE_HT_CONN_FILES_FLAG,\n",
    "            num_of_topics=NUM_OF_TOPICS, \n",
    "            top_no_word_filter=TOP_NO_WORD_FILTER, \n",
    "            top_ht_to_ignore=TOP_HT_TO_IGNORE,\n",
    "            graph_plot_cutoff_no_nodes=GRAPH_PLOT_CUTOFF_NO_NODES, \n",
    "            graph_plot_cutoff_no_edges=GRAPH_PLOT_CUTOFF_NO_EDGES,\n",
    "            create_graph_without_node_scale_flag=CREATE_GRAPH_WITHOUT_NODE_SCALE_FLAG, \n",
    "            create_graph_with_node_scale_flag=CREATE_GRAPH_WITH_NODE_SCALE_FLAG, \n",
    "            create_reduced_graph_flag=CREATE_REDUCED_GRAPH_FLAG,\n",
    "            reduced_graph_comty_contract_per=REDUCED_GRAPH_COMTY_PER,\n",
    "            reduced_graph_remove_edge_weight=REDUCED_GRAPH_REMOVE_EDGE_WEIGHT, \n",
    "            reduced_graph_remove_edges=REDUCED_GRAPH_REMOVE_EDGES_UNTIL_CUTOFF_FLAG,                            \n",
    "            top_degree_start=TOP_DEGREE_START, \n",
    "            top_degree_end=TOP_DEGREE_END, \n",
    "            period_top_degree_start=PERIOD_TOP_DEGREE_START, \n",
    "            period_top_degree_end=PERIOD_TOP_DEGREE_END, \n",
    "            commty_edge_size_cutoff=COMMTY_EDGE_SIZE_CUTOFF\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnalysis.edge_files_analysis(output_path=OUTPUT_PATH)\n",
    "\n",
    "print(\"**** END ****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual Analysis Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LDA Analysis files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnalysis.lda_analysis_files('D:\\\\Data\\\\MyFiles', startDate_filter='09/20/2020 00:00:00', endDate_filter='03/04/2021 00:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hashtag frequency Analysis files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnalysis.ht_analysis_files('D:\\\\Data\\\\MyFiles', startDate_filter='09/20/2020 00:00:00', endDate_filter='03/04/2021 00:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word frequency Analysis files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnalysis.words_analysis_files('D:\\\\Data\\\\MyFiles', startDate_filter='09/20/2020 00:00:00', endDate_filter='03/04/2021 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ... will add more soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
